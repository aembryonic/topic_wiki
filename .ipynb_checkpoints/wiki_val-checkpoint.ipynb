{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling per Grazia\n",
    "\n",
    "in questo notebook ci sono delle utils che ho usato per costruire il modellino. Nella repo, invece:\n",
    "- nodes-with... : il tuo csv\n",
    "- mallet-2.0.8: una versione del modello LDA modificata.\n",
    "- wiki_txt: i testi (in utf-8 encoding) delle pagine che mi hai indicato\n",
    "- links.json: un json (dizionario) chiave-valore/titolo-links nella pagina\n",
    "- bigram_model.pkl: un phraser (custom sul corpus in questione) che considera i bigrammi\n",
    "- trigram_model.pkl: un phraser (custom sul corpus in questione) che considera i trigrammi\n",
    "- wikitdm: ho modificato il progetto di un tizio che permetteva, a partire da una directory con dei txts, di creare un tdidf direttamente, ma non l'ho poi usato perchè è troppo basic. Lo aggiungo per completezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi, os\n",
    "import gensim\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from pyLDAvis import gensim as gm\n",
    "from gensim.corpora import TextCorpus, Dictionary\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/nicolo/Documenti/projects/wiki/wiki_txt/\"\n",
    "\n",
    "remove_title = [\"See also\", \n",
    "                \"References\", \n",
    "                \"Further reading\", \n",
    "                \"External links\", \n",
    "                \"History\", \n",
    "                \"Critiques\",\n",
    "                \"Notes\",\n",
    "                \"Publications\",\n",
    "                \"Controversies\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "path = [os.path.join(save_dir, el) for el in os.listdir(save_dir) if el[-3:] == \"txt\" ]\n",
    "mallet_path = \"/home/nicolo/Scaricati/mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = pd.read_csv(\"/home/nicolo/Documenti/projects/wiki/nodes-with-modularity-02122019csv.csv\")\n",
    "wiki_data_names = list(wiki_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia(language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pages(list_of_names:list, wiki_object, save_dir:str):\n",
    "    \n",
    "    wiki_json_data, fail = {}, []\n",
    "    \n",
    "    for wiki_page in list_of_names:\n",
    "        \n",
    "        try: \n",
    "            page = wiki_object.page(wiki_page)\n",
    "            format_name = page.title.replace(\" \", \"_\") +'.txt'\n",
    "            format_name = format_name.replace(\"/\",\"_\")\n",
    "            links = [link for link in list(page.links.keys()) if \":\" not in link]\n",
    "            with open(os.path.join(save_dir, format_name), \"w+\", encoding=\"utf-8\") as wiki_txt:\n",
    "                wiki_txt.write(page.text)\n",
    "\n",
    "            wiki_json_data.update({\"name\": page.title,\n",
    "                                   \"links\": links})\n",
    "        except Exception as e:\n",
    "            fail.append(wiki_page)\n",
    "            print(\"error {} in {}\".format(e, wiki_page))\n",
    "    \n",
    "    return wiki_json_data, fail\n",
    "\n",
    "def get_token(text_path: str, tokenizer, stop_words):\n",
    "\n",
    "    with open(text_path, encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    token = [tok.lower() for tok in tokenizer.tokenize(text) if tok not in stop_words and not tok.isdigit() and len(tok) >= 3]\n",
    "    return token\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute coherence for various number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dic)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [get_token(doc, tokenizer, stop_words) for doc in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phraser.load(\"/home/nicolo/Documenti/projects/wiki/trigram_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tok = [c[0] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary(corpus_tok)\n",
    "corpus_ = [dic.doc2bow(text) for text in corpus_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dic, corpus=corpus_, texts=docs, start=5, limit=20, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[2]\n",
    "#model_topics = optimal_model.show_topics(formatted=False)\n",
    "#print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(model_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.enable_notebook()\n",
    "#vis = gm.prepare(model_lda, corpus, dic)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tika",
   "language": "python",
   "name": "tika"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
