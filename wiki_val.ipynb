{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling per Grazia\n",
    "\n",
    "in questo notebook ci sono delle utils che ho usato per costruire il modellino. Nella repo, invece:\n",
    "- nodes-with... : il tuo csv\n",
    "- LDAmallet: modello originale fatto con mallet\n",
    "- LDA.model, LDA.model.id2word, ... : modelli convertiti per essere caricati con Gensim\n",
    "- mallet-2.0.8: una versione del modello LDA, ottimizzata in Java. [http://mallet.cs.umass.edu/topics.php]\n",
    "- topic_corpus.mm: il corpus serializzato\n",
    "- wiki_txt: i testi (in utf-8 encoding) delle pagine che mi hai indicato\n",
    "- links.json: un json (dizionario) chiave-valore/titolo-links nella pagina\n",
    "- bigram_model.pkl: un phraser (custom sul corpus in questione) che considera i bigrammi\n",
    "- trigram_model.pkl: un phraser (custom sul corpus in questione) che considera i trigrammi\n",
    "- wikitdm: ho modificato il progetto di un tizio che permetteva, a partire da una directory con dei txts, di creare un tdidf direttamente, ma non l'ho poi usato perchè è troppo basic. Lo aggiungo per completezza.\n",
    "\n",
    "#### Links\n",
    "\n",
    "- Coherence Topic Model: [http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi, os\n",
    "import gensim\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from pyLDAvis import gensim as gm\n",
    "from gensim.corpora import TextCorpus, Dictionary\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/nicolo/Documenti/projects/wiki/wiki_txt/\"\n",
    "\n",
    "remove_title = [\"See also\", \n",
    "                \"References\", \n",
    "                \"Further reading\", \n",
    "                \"External links\", \n",
    "                \"History\", \n",
    "                \"Critiques\",\n",
    "                \"Notes\",\n",
    "                \"Publications\",\n",
    "                \"Controversies\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "path = [os.path.join(save_dir, el) for el in os.listdir(save_dir) if el[-3:] == \"txt\" ]\n",
    "mallet_path = \"/home/nicolo/Scaricati/mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = pd.read_csv(\"/home/nicolo/Documenti/projects/wiki/nodes-with-modularity-02122019csv.csv\")\n",
    "wiki_data_names = list(wiki_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia(language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pages(list_of_names:list, wiki_object, save_dir:str):\n",
    "    \n",
    "    wiki_json_data, fail = {}, []\n",
    "    \n",
    "    for wiki_page in list_of_names:\n",
    "        \n",
    "        try: \n",
    "            page = wiki_object.page(wiki_page)\n",
    "            format_name = page.title.replace(\" \", \"_\") +'.txt'\n",
    "            format_name = format_name.replace(\"/\",\"_\")\n",
    "            links = [link for link in list(page.links.keys()) if \":\" not in link]\n",
    "            with open(os.path.join(save_dir, format_name), \"w+\", encoding=\"utf-8\") as wiki_txt:\n",
    "                wiki_txt.write(page.text)\n",
    "\n",
    "            wiki_json_data.update({\"name\": page.title,\n",
    "                                   \"links\": links})\n",
    "        except Exception as e:\n",
    "            fail.append(wiki_page)\n",
    "            print(\"error {} in {}\".format(e, wiki_page))\n",
    "    \n",
    "    return wiki_json_data, fail\n",
    "\n",
    "def get_token(text_path: str, tokenizer, stop_words):\n",
    "\n",
    "    with open(text_path, encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    token = [tok.lower() for tok in tokenizer.tokenize(text) if tok not in stop_words and not tok.isdigit() and len(tok) >= 3]\n",
    "    return token\n",
    "\n",
    "def compute_coherence_values(mallet_path, dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute coherence for various number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dic)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', \"Tokens\"]\n",
    "\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "def create_models(trigrams_phraser,\n",
    "                  mallet_path,\n",
    "                  docs_path, \n",
    "                  tokenizer, \n",
    "                  stop_words, \n",
    "                  thres=20,\n",
    "                  num_topics_start=5,\n",
    "                  num_topics_end=25,\n",
    "                  step=5):\n",
    "    \n",
    "    trigrams = Phraser.load(trigrams_phraser)\n",
    "    docs = [get_token(doc, tokenizer, stop_words) for doc in docs_path]\n",
    "    corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=thres]\n",
    "    corpus_tok, ids = [c[0] for c in corpus], [c[1] for c in corpus]\n",
    "    dic = Dictionary(corpus_tok)\n",
    "    corpus_ = [dic.doc2bow(text) for text in corpus_tok]\n",
    "    model_list, coherence_values = compute_coherence_values(mallet_path,\n",
    "                                                            dictionary=dic, \n",
    "                                                            corpus=corpus_, \n",
    "                                                            texts=docs, \n",
    "                                                            start=num_topics_start, \n",
    "                                                            limit=num_topics_end, \n",
    "                                                            step=step)\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [get_token(doc, tokenizer, stop_words) for doc in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phraser.load(\"/home/nicolo/Documenti/projects/wiki/trigram_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tok = [c[0] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary(corpus_tok)\n",
    "corpus_ = [dic.doc2bow(text) for text in corpus_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(mallet_path, dictionary=dic, corpus=corpus_, texts=docs, start=20, limit=30, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.1145788385600834, -1.0348815774143976]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[1]\n",
    "optimal_model.save(\"/home/nicolo/Documenti/projects/wiki/LDAmallet_25\")\n",
    "#model_topics = optimal_model.show_topics(formatted=False)\n",
    "#print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_25 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(optimal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_25.save(\"/home/nicolo/Documenti/projects/wiki/LDA_25.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = format_topics_sentences(ldamodel_25, corpus_, corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.Series([p for i, p in enumerate(path) if i in ids], name=\"Page\")\n",
    "tab = pd.concat([tab, contents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8707</td>\n",
       "      <td>species, plants, plant, animals, food, animal,...</td>\n",
       "      <td>Botanical_terms.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8612</td>\n",
       "      <td>species, plants, plant, animals, food, animal,...</td>\n",
       "      <td>Chaetoceros.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>species, plants, plant, animals, food, animal,...</td>\n",
       "      <td>Zoid.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8377</td>\n",
       "      <td>species, plants, plant, animals, food, animal,...</td>\n",
       "      <td>Ants.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8377</td>\n",
       "      <td>species, plants, plant, animals, food, animal,...</td>\n",
       "      <td>ants.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>economic, government, law, public, market, com...</td>\n",
       "      <td>Supervisory_board.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>economic, government, law, public, market, com...</td>\n",
       "      <td>Limited_liability_company.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.8450</td>\n",
       "      <td>economic, government, law, public, market, com...</td>\n",
       "      <td>Liquidation.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>economic, government, law, public, market, com...</td>\n",
       "      <td>Joint-stock_company.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>economic, government, law, public, market, com...</td>\n",
       "      <td>Insider_dealing.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          0.0              0.8707   \n",
       "1          0.0              0.8612   \n",
       "2          0.0              0.8583   \n",
       "3          0.0              0.8377   \n",
       "4          0.0              0.8377   \n",
       "..         ...                 ...   \n",
       "145       14.0              0.8452   \n",
       "146       14.0              0.8452   \n",
       "147       14.0              0.8450   \n",
       "148       14.0              0.8251   \n",
       "149       14.0              0.8228   \n",
       "\n",
       "                                        Topic_Keywords  \\\n",
       "0    species, plants, plant, animals, food, animal,...   \n",
       "1    species, plants, plant, animals, food, animal,...   \n",
       "2    species, plants, plant, animals, food, animal,...   \n",
       "3    species, plants, plant, animals, food, animal,...   \n",
       "4    species, plants, plant, animals, food, animal,...   \n",
       "..                                                 ...   \n",
       "145  economic, government, law, public, market, com...   \n",
       "146  economic, government, law, public, market, com...   \n",
       "147  economic, government, law, public, market, com...   \n",
       "148  economic, government, law, public, market, com...   \n",
       "149  economic, government, law, public, market, com...   \n",
       "\n",
       "                              Page  \n",
       "0              Botanical_terms.txt  \n",
       "1                  Chaetoceros.txt  \n",
       "2                         Zoid.txt  \n",
       "3                         Ants.txt  \n",
       "4                         ants.txt  \n",
       "..                             ...  \n",
       "145          Supervisory_board.txt  \n",
       "146  Limited_liability_company.txt  \n",
       "147                Liquidation.txt  \n",
       "148        Joint-stock_company.txt  \n",
       "149            Insider_dealing.txt  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sort = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = new_tab.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sort = pd.concat([sent_topics_sort, \n",
    "                                 grp.sort_values(['Perc_Contribution'], ascending=False).head(10)], \n",
    "                                 axis=0)\n",
    "    \n",
    "sent_topics_sort.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sort.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Topic_Keywords\", \"Page\"]\n",
    "\n",
    "sent_topics_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sort.to_csv(\"/home/nicolo/Documenti/projects/wiki/most_important_pages.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tika",
   "language": "python",
   "name": "tika"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
