{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling per Grazia\n",
    "\n",
    "in questo notebook ci sono delle utils che ho usato per costruire il modellino. Nella repo, invece:\n",
    "- nodes-with... : il tuo csv\n",
    "- LDA_15: modello con 15 topics\n",
    "- LDA_25: modello con 25 topics\n",
    "- mallet-2.0.8: una versione del modello LDA, ottimizzata in Java. [http://mallet.cs.umass.edu/topics.php]\n",
    "- corpus/topic_corpus.mm: il corpus serializzato\n",
    "- wiki_txt: i testi (in utf-8 encoding) delle pagine che mi hai indicato\n",
    "- links.json: un json (dizionario) chiave-valore/titolo-links nella pagina\n",
    "- phrasers/bigram_model.pkl: un phraser (custom sul corpus in questione) che considera i bigrammi\n",
    "- phrasers/trigram_model.pkl: un phraser (custom sul corpus in questione) che considera i trigrammi\n",
    "- wikitdm: ho modificato il progetto di un tizio che permetteva, a partire da una directory con dei txts, di creare un tdidf direttamente, ma non l'ho poi usato perchè è troppo basic. Lo aggiungo per completezza.\n",
    "\n",
    "#### Links\n",
    "\n",
    "- Coherence Topic Model: [http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi, os\n",
    "import matplotlib.colors as mcolors\n",
    "import gensim, pyLDAvis\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from pyLDAvis import gensim as gm\n",
    "from gensim.corpora import TextCorpus, Dictionary\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/nicolo/Documenti/projects/wiki/wiki_txt/\"\n",
    "\n",
    "remove_title = [\"See also\", \n",
    "                \"References\", \n",
    "                \"Further reading\", \n",
    "                \"External links\", \n",
    "                \"History\", \n",
    "                \"Critiques\",\n",
    "                \"Notes\",\n",
    "                \"Publications\",\n",
    "                \"Controversies\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "path = [os.path.join(save_dir, el) for el in os.listdir(save_dir) if el[-3:] == \"txt\" ]\n",
    "mallet_path = \"/home/nicolo/Scaricati/mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = pd.read_csv(\"/home/nicolo/Documenti/projects/wiki/nodes-with-modularity-02122019csv.csv\")\n",
    "wiki_data_names = list(wiki_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia(language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pages(list_of_names:list, wiki_object, save_dir:str):\n",
    "    \n",
    "    wiki_json_data, fail = {}, []\n",
    "    \n",
    "    for wiki_page in list_of_names:\n",
    "        \n",
    "        try: \n",
    "            page = wiki_object.page(wiki_page)\n",
    "            format_name = page.title.replace(\" \", \"_\") +'.txt'\n",
    "            format_name = format_name.replace(\"/\",\"_\")\n",
    "            links = [link for link in list(page.links.keys()) if \":\" not in link]\n",
    "            with open(os.path.join(save_dir, format_name), \"w+\", encoding=\"utf-8\") as wiki_txt:\n",
    "                wiki_txt.write(page.text)\n",
    "\n",
    "            wiki_json_data.update({\"name\": page.title,\n",
    "                                   \"links\": links})\n",
    "        except Exception as e:\n",
    "            fail.append(wiki_page)\n",
    "            print(\"error {} in {}\".format(e, wiki_page))\n",
    "    \n",
    "    return wiki_json_data, fail\n",
    "\n",
    "def get_token(text_path: str, tokenizer, stop_words):\n",
    "\n",
    "    with open(text_path, encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    token = [tok.lower() for tok in tokenizer.tokenize(text) if tok not in stop_words and not tok.isdigit() and len(tok) >= 3]\n",
    "    return token\n",
    "\n",
    "def compute_coherence_values(mallet_path, dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute coherence for various number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dic)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "def create_models(trigrams_phraser,\n",
    "                  mallet_path,\n",
    "                  docs_path, \n",
    "                  tokenizer, \n",
    "                  stop_words, \n",
    "                  thres=20,\n",
    "                  num_topics_start=5,\n",
    "                  num_topics_end=25,\n",
    "                  step=5):\n",
    "    \n",
    "    trigrams = Phraser.load(trigrams_phraser)\n",
    "    docs = [get_token(doc, tokenizer, stop_words) for doc in docs_path]\n",
    "    corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=thres]\n",
    "    corpus_tok, ids = [c[0] for c in corpus], [c[1] for c in corpus]\n",
    "    dic = Dictionary(corpus_tok)\n",
    "    corpus_ = [dic.doc2bow(text) for text in corpus_tok]\n",
    "    model_list, coherence_values = compute_coherence_values(mallet_path,\n",
    "                                                            dictionary=dic, \n",
    "                                                            corpus=corpus_, \n",
    "                                                            texts=docs, \n",
    "                                                            start=num_topics_start, \n",
    "                                                            limit=num_topics_end, \n",
    "                                                            step=step)\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [get_token(doc, tokenizer, stop_words) for doc in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phraser.load(\"/home/nicolo/Documenti/projects/wiki/trigram_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tok = [c[0] for c in corpus]\n",
    "ids = [c[1] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary(corpus_tok)\n",
    "corpus_ = [dic.doc2bow(text) for text in corpus_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(mallet_path, dictionary=dic, corpus=corpus_, texts=docs, start=20, limit=30, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[1]\n",
    "optimal_model.save(\"/home/nicolo/Documenti/projects/wiki/LDAmallet_25\")\n",
    "#model_topics = optimal_model.show_topics(formatted=False)\n",
    "#print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_25 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(optimal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_25.save(\"/home/nicolo/Documenti/projects/wiki/LDA_25.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = format_topics_sentences(ldamodel_25, corpus_, corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_path = [p for i, p in enumerate(path) if i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tab.drop([0, \"Page\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.Series([p for i, p in enumerate(path)], name=\"Page\")\n",
    "tab_ = pd.concat([tab, contents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_ = tab_.rename(columns={0:\"Tokens\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sort = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = tab_.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sort = pd.concat([sent_topics_sort, \n",
    "                                 grp.sort_values(['Perc_Contribution'], ascending=False).head(15)], \n",
    "                                 axis=0)\n",
    "    \n",
    "sent_topics_sort.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sort.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Topic_Keywords\", \"Page\"]\n",
    "\n",
    "sent_topics_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_.to_csv(\"/home/nicolo/Documenti/projects/wiki/topic_pages_25.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = ldamodel_25.show_topics(formatted=False, num_topics=25)[17:]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    if i == len(topics): break\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i+17), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel.load(\"/home/nicolo/Documenti/projects/wiki/LDA_25/LDA_25.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gensim.corpora.MmCorpus(\"/home/nicolo/Documenti/projects/wiki/corpus/topic_corpus.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary.load(\"/home/nicolo/Documenti/projects/wiki/dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolo/anaconda3/envs/tika/lib/python3.6/site-packages/pyLDAvis/_prepare.py:223: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = gm.prepare(lda_model, corpus, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tika",
   "language": "python",
   "name": "tika"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
