{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling per Grazia\n",
    "\n",
    "in questo notebook ci sono delle utils che ho usato per costruire il modellino. Nella repo, invece:\n",
    "- nodes-with... : il tuo csv\n",
    "- LDAmallet: modello originale fatto con mallet\n",
    "- LDA.model, LDA.model.id2word, ... : modelli convertiti per essere caricati con Gensim\n",
    "- mallet-2.0.8: una versione del modello LDA, ottimizzata in Java. [http://mallet.cs.umass.edu/topics.php]\n",
    "- topic_corpus.mm: il corpus serializzato\n",
    "- wiki_txt: i testi (in utf-8 encoding) delle pagine che mi hai indicato\n",
    "- links.json: un json (dizionario) chiave-valore/titolo-links nella pagina\n",
    "- bigram_model.pkl: un phraser (custom sul corpus in questione) che considera i bigrammi\n",
    "- trigram_model.pkl: un phraser (custom sul corpus in questione) che considera i trigrammi\n",
    "- wikitdm: ho modificato il progetto di un tizio che permetteva, a partire da una directory con dei txts, di creare un tdidf direttamente, ma non l'ho poi usato perchè è troppo basic. Lo aggiungo per completezza.\n",
    "\n",
    "#### Links\n",
    "\n",
    "- Coherence Topic Model: [http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipediaapi, os\n",
    "import gensim\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from pyLDAvis import gensim as gm\n",
    "from gensim.corpora import TextCorpus, Dictionary\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/nicolo/Documenti/projects/wiki/wiki_txt/\"\n",
    "\n",
    "remove_title = [\"See also\", \n",
    "                \"References\", \n",
    "                \"Further reading\", \n",
    "                \"External links\", \n",
    "                \"History\", \n",
    "                \"Critiques\",\n",
    "                \"Notes\",\n",
    "                \"Publications\",\n",
    "                \"Controversies\"]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "path = [os.path.join(save_dir, el) for el in os.listdir(save_dir) if el[-3:] == \"txt\" ]\n",
    "mallet_path = \"/home/nicolo/Scaricati/mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = pd.read_csv(\"/home/nicolo/Documenti/projects/wiki/nodes-with-modularity-02122019csv.csv\")\n",
    "wiki_data_names = list(wiki_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia(language=\"en\", extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pages(list_of_names:list, wiki_object, save_dir:str):\n",
    "    \n",
    "    wiki_json_data, fail = {}, []\n",
    "    \n",
    "    for wiki_page in list_of_names:\n",
    "        \n",
    "        try: \n",
    "            page = wiki_object.page(wiki_page)\n",
    "            format_name = page.title.replace(\" \", \"_\") +'.txt'\n",
    "            format_name = format_name.replace(\"/\",\"_\")\n",
    "            links = [link for link in list(page.links.keys()) if \":\" not in link]\n",
    "            with open(os.path.join(save_dir, format_name), \"w+\", encoding=\"utf-8\") as wiki_txt:\n",
    "                wiki_txt.write(page.text)\n",
    "\n",
    "            wiki_json_data.update({\"name\": page.title,\n",
    "                                   \"links\": links})\n",
    "        except Exception as e:\n",
    "            fail.append(wiki_page)\n",
    "            print(\"error {} in {}\".format(e, wiki_page))\n",
    "    \n",
    "    return wiki_json_data, fail\n",
    "\n",
    "def get_token(text_path: str, tokenizer, stop_words):\n",
    "\n",
    "    with open(text_path, encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    token = [tok.lower() for tok in tokenizer.tokenize(text) if tok not in stop_words and not tok.isdigit() and len(tok) >= 3]\n",
    "    return token\n",
    "\n",
    "def compute_coherence_values(mallet_path, dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute coherence for various number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dic)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', \"Tokens\"]\n",
    "\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "def create_models(trigrams_phraser,\n",
    "                  mallet_path,\n",
    "                  docs_path, \n",
    "                  tokenizer, \n",
    "                  stop_words, \n",
    "                  thres=20,\n",
    "                  num_topics_start=5,\n",
    "                  num_topics_end=25,\n",
    "                  step=5):\n",
    "    \n",
    "    trigrams = Phraser.load(trigrams_phraser)\n",
    "    docs = [get_token(doc, tokenizer, stop_words) for doc in docs_path]\n",
    "    corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=thres]\n",
    "    corpus_tok, ids = [c[0] for c in corpus], [c[1] for c in corpus]\n",
    "    dic = Dictionary(corpus_tok)\n",
    "    corpus_ = [dic.doc2bow(text) for text in corpus_tok]\n",
    "    model_list, coherence_values = compute_coherence_values(mallet_path,\n",
    "                                                            dictionary=dic, \n",
    "                                                            corpus=corpus_, \n",
    "                                                            texts=docs, \n",
    "                                                            start=num_topics_start, \n",
    "                                                            limit=num_topics_end, \n",
    "                                                            step=step)\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [get_token(doc, tokenizer, stop_words) for doc in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phraser.load(\"/home/nicolo/Documenti/projects/wiki/trigram_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[trigrams[doc], i] for i, doc in enumerate(docs) if len(doc)>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tok = [c[0] for c in corpus]\n",
    "ids = [c[1] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Dictionary(corpus_tok)\n",
    "corpus_ = [dic.doc2bow(text) for text in corpus_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(mallet_path, dictionary=dic, corpus=corpus_, texts=docs, start=20, limit=30, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.1145788385600834, -1.0348815774143976]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[1]\n",
    "optimal_model.save(\"/home/nicolo/Documenti/projects/wiki/LDAmallet_25\")\n",
    "#model_topics = optimal_model.show_topics(formatted=False)\n",
    "#print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_25 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(optimal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_25.save(\"/home/nicolo/Documenti/projects/wiki/LDA_25.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = format_topics_sentences(ldamodel_25, corpus_, corpus_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_path = [p for i, p in enumerate(path) if i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = tab.drop([0, \"Page\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.Series([p for i, p in enumerate(path)], name=\"Page\")\n",
    "tab = pd.concat([tab, contents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8063</td>\n",
       "      <td>food, production, plant, soil, plants, crops, ...</td>\n",
       "      <td>Flour.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8063</td>\n",
       "      <td>food, production, plant, soil, plants, crops, ...</td>\n",
       "      <td>flour.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>food, production, plant, soil, plants, crops, ...</td>\n",
       "      <td>fodder.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7332</td>\n",
       "      <td>food, production, plant, soil, plants, crops, ...</td>\n",
       "      <td>Barley.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7256</td>\n",
       "      <td>food, production, plant, soil, plants, crops, ...</td>\n",
       "      <td>forage.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>cells, cell, health, disease, human, body, med...</td>\n",
       "      <td>breast_cancer.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.8158</td>\n",
       "      <td>cells, cell, health, disease, human, body, med...</td>\n",
       "      <td>Staphylococcus_aureus.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.8155</td>\n",
       "      <td>cells, cell, health, disease, human, body, med...</td>\n",
       "      <td>birth_defects.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.8090</td>\n",
       "      <td>cells, cell, health, disease, human, body, med...</td>\n",
       "      <td>coagulation.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>cells, cell, health, disease, human, body, med...</td>\n",
       "      <td>Diabetes.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic_Num  Topic_Perc_Contrib  \\\n",
       "0          0.0              0.8063   \n",
       "1          0.0              0.8063   \n",
       "2          0.0              0.7391   \n",
       "3          0.0              0.7332   \n",
       "4          0.0              0.7256   \n",
       "..         ...                 ...   \n",
       "370       24.0              0.8193   \n",
       "371       24.0              0.8158   \n",
       "372       24.0              0.8155   \n",
       "373       24.0              0.8090   \n",
       "374       24.0              0.8062   \n",
       "\n",
       "                                        Topic_Keywords  \\\n",
       "0    food, production, plant, soil, plants, crops, ...   \n",
       "1    food, production, plant, soil, plants, crops, ...   \n",
       "2    food, production, plant, soil, plants, crops, ...   \n",
       "3    food, production, plant, soil, plants, crops, ...   \n",
       "4    food, production, plant, soil, plants, crops, ...   \n",
       "..                                                 ...   \n",
       "370  cells, cell, health, disease, human, body, med...   \n",
       "371  cells, cell, health, disease, human, body, med...   \n",
       "372  cells, cell, health, disease, human, body, med...   \n",
       "373  cells, cell, health, disease, human, body, med...   \n",
       "374  cells, cell, health, disease, human, body, med...   \n",
       "\n",
       "                          Page  \n",
       "0                    Flour.txt  \n",
       "1                    flour.txt  \n",
       "2                   fodder.txt  \n",
       "3                   Barley.txt  \n",
       "4                   forage.txt  \n",
       "..                         ...  \n",
       "370          breast_cancer.txt  \n",
       "371  Staphylococcus_aureus.txt  \n",
       "372          birth_defects.txt  \n",
       "373            coagulation.txt  \n",
       "374               Diabetes.txt  \n",
       "\n",
       "[375 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sort = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = tab.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sort = pd.concat([sent_topics_sort, \n",
    "                                 grp.sort_values(['Perc_Contribution'], ascending=False).head(15)], \n",
    "                                 axis=0)\n",
    "    \n",
    "sent_topics_sort.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sort.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Topic_Keywords\", \"Page\"]\n",
    "\n",
    "sent_topics_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sort.to_csv(\"/home/nicolo/Documenti/projects/wiki/most_important_pages_25.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tika",
   "language": "python",
   "name": "tika"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
